Problem (benchmarking_script): 4 points

(a) Write a script to perform basic end-to-end benchmarking of the forward and backward passes in your model. Specifically, your script should support the following:
• Given hyperparameters (e.g., number of layers), initialize a model.
• Generate a random batch of data.
• Run w warm-up steps (before you start measuring time), then time the execution of n steps (either only forward, or both forward and backward passes, depending on an argument). For timing, you can use the Python timeit module (e.g., either using the timeit function, or using timeit.default_timer(), which gives you the system’s highest resolution clock, thus a better default for benchmarking than time.time()).
• Call torch.cuda.synchronize() after each step.

Deliverable: A script that will initialize a basics Transformer model with the given hyperparameters, create a random batch of data, and time forward and backward passes.

run commands: 
only forward pass:

uv run python cs336_systems/benchmarking_script.py --n_layers 4 --d_model 512 --n_heads 16 --batch_size 64

tarnsformerLM config: {'vocab_size': 10000, 'context_length': 256, 'num_layers': 4, 'd_model': 512, 'num_heads': 16, 'd_ff': 1344, 'device': device(type='cuda')}
model parameters: 22.696M
performing warmup for 5 steps...
benchmarking...
results (forward only):
mean time: 106.39 ms
std dev: 1.49 ms


both forward and backward:
uv run python cs336_systems/benchmarking_script.py --n_layers 4 --d_model 512 --n_heads 16 --batch_size 64 --backward


tarnsformerLM config: {'vocab_size': 10000, 'context_length': 256, 'num_layers': 4, 'd_model': 512, 'num_heads': 16, 'd_ff': 1344, 'device': device(type='cuda')}
model parameters: 22.696M
performing warmup for 5 steps...
benchmarking...
results (forward + backward):
mean time: 352.83 ms
std dev: 1.69 ms


(b) Time the forward and backward passes for the model sizes described in §1.1.2. Use 5 warmup steps and compute the average and standard deviation of timings over 10 measurement steps. How long does a forward pass take? How about a backward pass? Do you see high variability across measurements, or is the standard deviation small?

1. small:

uv run python cs336_systems/benchmarking_script.py --d_model 768 --d_ff 3072 --n_layers 12 --n_heads 12 --batch_size 4 --vocab_size 10000 --warmup_steps 5 --n_steps 10

tarnsformerLM config: {'vocab_size': 10000, 'context_length': 256, 'num_layers': 12, 'd_model': 768, 'num_heads': 12, 'd_ff': 3072, 'device': device(type='cuda')}
model parameters: 128.625M
performing warmup for 5 steps...
benchmarking...
results (forward only):
mean time: 34.57 ms
std dev: 0.22 ms


tarnsformerLM config: {'vocab_size': 10000, 'context_length': 256, 'num_layers': 12, 'd_model': 768, 'num_heads': 12, 'd_ff': 3072, 'device': device(type='cuda')}
model parameters: 128.625M
performing warmup for 5 steps...
benchmarking...
results (forward + backward):
mean time: 108.51 ms
std dev: 1.89 ms


2. medium

uv run python cs336_systems/benchmarking_script.py --d_model 1024 --d_ff 4096 --n_layers 24 --n_heads 16 --batch_size 4 --vocab_size 10000 --warmup_steps 5 --n_steps 10


tarnsformerLM config: {'vocab_size': 10000, 'context_length': 256, 'num_layers': 24, 'd_model': 1024, 'num_heads': 16, 'd_ff': 4096, 'device': device(type='cuda')}
model parameters: 423.183M
performing warmup for 5 steps...
benchmarking...
results (forward only):
mean time: 109.64 ms
std dev: 3.14 ms


tarnsformerLM config: {'vocab_size': 10000, 'context_length': 256, 'num_layers': 24, 'd_model': 1024, 'num_heads': 16, 'd_ff': 4096, 'device': device(type='cuda')}
model parameters: 423.183M
performing warmup for 5 steps...
benchmarking...
results (forward + backward):
mean time: 320.68 ms
std dev: 1.05 ms


3. large

uv run python cs336_systems/benchmarking_script.py --d_model 1280 --d_ff 5120 --n_layers 36 --n_heads 20 --batch_size 4 --vocab_size 10000 --warmup_steps 5 --n_steps 10

tarnsformerLM config: {'vocab_size': 10000, 'context_length': 256, 'num_layers': 36, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120, 'device': device(type='cuda')}
model parameters: 969.412M
performing warmup for 5 steps...
benchmarking...
results (forward only):
mean time: 232.65 ms
std dev: 2.07 ms


tarnsformerLM config: {'vocab_size': 10000, 'context_length': 256, 'num_layers': 36, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120, 'device': device(type='cuda')}
model parameters: 969.412M
performing warmup for 5 steps...
benchmarking...
results (forward + backward):
mean time: 706.12 ms
std dev: 1.25 ms


4. xl

uv run python cs336_systems/benchmarking_script.py --d_model 1600 --d_ff 6400 --n_layers 48 --n_heads 25 --batch_size 4 --vocab_size 10000 --warmup_steps 5 --n_steps 10


tarnsformerLM config: {'vocab_size': 10000, 'context_length': 256, 'num_layers': 48, 'd_model': 1600, 'num_heads': 25, 'd_ff': 6400, 'device': device(type='cuda')}
model parameters: 1998.235M
performing warmup for 5 steps...
benchmarking...
results (forward only):
mean time: 482.49 ms
std dev: 2.92 ms


tarnsformerLM config: {'vocab_size': 10000, 'context_length': 256, 'num_layers': 48, 'd_model': 1600, 'num_heads': 25, 'd_ff': 6400, 'device': device(type='cuda')}
model parameters: 1998.235M
performing warmup for 5 steps...
benchmarking...
results (forward + backward):
mean time: 1435.72 ms
std dev: 5.31 ms


5. 2.35B

uv run python cs336_systems/benchmarking_script.py --d_model 3200 --d_ff 25600 --n_layers 8 --n_heads 4 --batch_size 4 --vocab_size 10000 --warmup_steps 0 --n_steps 10

tarnsformerLM config: {'vocab_size': 10000, 'context_length': 256, 'num_layers': 8, 'd_model': 3200, 'num_heads': 4, 'd_ff': 25600, 'device': device(type='cuda')}
model parameters: 2357.814M
performing warmup for 5 steps...
benchmarking...
results (forward only):
mean time: 458.62 ms
std dev: 2.12 ms


tarnsformerLM config: {'vocab_size': 10000, 'context_length': 256, 'num_layers': 8, 'd_model': 3200, 'num_heads': 4, 'd_ff': 25600, 'device': device(type='cuda')}
model parameters: 2357.814M
performing warmup for 5 steps...
benchmarking...
results (forward + backward):
mean time: 1268.54 ms
std dev: 3.52 ms


| Model  | Fwd (ms) | Fwd+Bwd (ms) | Ratio  |
| ------ | -------- | ------------ | ------ |
| small  | 34.6     | 108.5        | ~3.14× |
| medium | 109.6    | 320.7        | ~2.9×  |
| large  | 232.6    | 706.1        | ~3.03× |
| xl     | 482.5    | 1435.7       | ~2.97× |
| 2.35B  | 458.6    | 1268.5       | ~2.77× |

~3x

(c) One caveat of benchmarking is not performing the warm-up steps. Repeat your analysis without the warm-up steps. How does this affect your results? Why do you think this happens? Also try to run the script with 1 or 2 warm-up steps. Why might the result still be different?

uv run python cs336_systems/benchmarking_script.py --d_model 3200 --d_ff 25600 --n_layers 8 --n_heads 4 --batch_size 4 --vocab_size 10000 --warmup_steps 0 --n_steps 10

tarnsformerLM config: {'vocab_size': 10000, 'context_length': 256, 'num_layers': 8, 'd_model': 3200, 'num_heads': 4, 'd_ff': 25600, 'device': device(type='cuda')}
model parameters: 2357.814M
benchmarking...
results (forward only):
mean time: 548.18 ms
std dev: 273.68 ms


tarnsformerLM config: {'vocab_size': 10000, 'context_length': 256, 'num_layers': 8, 'd_model': 3200, 'num_heads': 4, 'd_ff': 25600, 'device': device(type='cuda')}
model parameters: 2357.814M
benchmarking...
results (forward + backward):
mean time: 1362.45 ms
std dev: 309.48 ms

-> high standard deviation

---

Problem (nsys_profile): 5 points

Profile your forward pass, backward pass, and optimizer step using nsys with each of the model sizes described in Table 1 and context lengths of 128, 256, 512 and 1024 (you may run out of memory with some of these context lengths for the larger models, in which case just note it in your report).

commands:

nsys profile python -c "import torch; a=torch.randn(1000,1000,device='cuda'); b=a@a; torch.cuda.synchronize()"


uv run nsys profile -o fwd_profile --force-overwrite true python cs336_systems/nsys_profile.py --d_model 768 --d_ff 3072 --n_layers 12 --n_heads 12 --batch_size 4 --vocab_size 10000 --warmup_steps 5 --n_steps 10


uv run nsys profile -o train_step_profile --force-overwrite true python cs336_systems/nsys_profile.py --include_optimizer --d_model 768 --d_ff 3072 --n_layers 12 --n_heads 12 --batch_size 4 --vocab_size 10000 --warmup_steps 5 --n_steps 10


(a) What is the total time spent on your forward pass? Does it match what we had measured before with the Python standard library?
Deliverable: A 1-2 sentence response.

yes (within a few ms)

uv run nsys profile -o train_step_profile --force-overwrite true python cs336_systems/nsys_profile.py --include_optimizer --d_model 768 --d_ff 3072 --n_layers 12 --n_heads 12 --batch_size 4 --vocab_size 10000 --warmup_steps 5 --n_steps 10

1. large model:


uv run nsys profile -o fwd_profile_2_35_b --force-overwrite true python cs336_systems/nsys_profile.py --d_model 1280 --d_ff 5120 --n_layers 36 --n_heads 20 --batch_size 4 --vocab_size 10000 --warmup_steps 5 --n_steps 10

results (forward only):
mean time: 234.07 ms
std dev: 1.92 ms

uv run nsys profile -o fwd_bwd_profile_2_35_b --force-overwrite true python cs336_systems/nsys_profile.py --d_model 1280 --d_ff 5120 --n_layers 36 --n_heads 20 --batch_size 4 --vocab_size 10000 --warmup_steps 5 --n_steps 10 --backward

results (foward + backward):
mean time: 709.57 ms
std dev: 1.48 ms

uv run nsys profile -o fwd_bwd_opt_profile_2_35_b --force-overwrite true python cs336_systems/nsys_profile.py --d_model 1280 --d_ff 5120 --n_layers 36 --n_heads 20 --batch_size 4 --vocab_size 10000 --warmup_steps 5 --n_steps 10 --backward --include_optimizer

results (forward + backward + optimizer):
mean time: 867.70 ms
std dev: 1.82 ms

(b) What CUDA kernel takes the most cumulative GPU time during the forward pass? How many times is this kernel invoked during a single forward pass of your model? Is it the same kernel that takes the most runtime when you do both forward and backward passes? (Hint: look at the “CUDA GPU Kernel Summary” under “Stats Systems View”, and filter using NVTX ranges to identify which parts of the model are responsible for which kernels.)
Deliverable: A 1-2 sentence response.

fwd:
Time	Total Time	Instances	Avg	Med	Min	Max	StdDev	Name
70.9%	83.312 ms	96	867.838 μs	1.056 ms	285.984 μs	1.091 ms	304.322 μs	ampere_sgemm_128x64_tn


(c) Although the vast majority of FLOPs take place in matrix multiplications, you will notice that several other kernels still take a non-trivial amount of the overall runtime. What other kernels besides matrix multiplies do you see accounting for non-trivial CUDA runtime in the forward pass?
Deliverable: A 1-2 sentence response.

some other kernals in fwd:
Time	Total Time	Instances	Avg	Med	Min	Max	StdDev	Name
4.4%	4.968 ms	38	130.747 μs	130.736 μs	128.896 μs	134.177 μs	870 ns	void at::native::vectorized_elementwise_kernel<(int)4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float>>, std::array<char *, (unsigned long)3>>(int, T2, T3)

Time	Total Time	Instances	Avg	Med	Min	Max	StdDev	Name
3.5%	4.039 ms	195	20.713 μs	19.872 μs	17.824 μs	25.792 μs	2.389 μs	void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)


(d) Profile running one complete training step with your implementation of AdamW (i.e., the forward pass, computing the loss and running a backward pass, and finally an optimizer step, as you’d do during training). How does the fraction of time spent on matrix multiplication change, compared to doing inference (forward pass only)? How about other kernels?
Deliverable: A 1-2 sentence response.

fwd only:
Time	Total Time	Instances	Avg	Med	Min	Max	StdDev	Name
70.3%	80.209 ms	96	835.510 μs	920.658 μs	242.241 μs	1.109 ms	301.794 μs	ampere_sgemm_128x64_tn

fwd + bwd + opt:
Time	Total Time	Instances	Avg	Med	Min	Max	StdDev	Name
18.4%	156.201 ms	181	862.990 μs	972.355 μs	242.241 μs	2.167 ms	320.811 μs	ampere_sgemm_128x64_tn

bwd, opt introduced memory-bound operations


(e) Compare the runtime of the softmax operation versus the matrix multiplication operations within the self-attention layer of your model during a forward pass. How does the difference in runtimes compare to the difference in FLOPs?
Deliverable: A 1-2 sentence response.

computing attention scores: [126.959 * 1e-6s]
Time	Total Time	Instances	Avg	Med	Min	Max	StdDev	Name
87.7%	715.426 μs	1	715.426 μs	715.426 μs	715.426 μs	715.426 μs	0 ns	ampere_sgemm_128x64_tn

masking: [83.434 * 1e-6s]
Time	Total Time	Instances	Avg	Med	Min	Max	StdDev	Name
19.3%	17.888 μs	1	17.888 μs	17.888 μs	17.888 μs	17.888 μs	0 ns	void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)

computing softmax: [112.235 * 1e-6s]
Time	Total Time	Instances	Avg	Med	Min	Max	StdDev	Name
58.6%	79.296 μs	4	19.824 μs	19.776 μs	18.304 μs	21.440 μs	1.285 μs	void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)

final mat_mul: [110.539 * 1e-6s]
Time	Total Time	Instances	Avg	Med	Min	Max	StdDev	Name
58.2%	85.856 μs	1	85.856 μs	85.856 μs	85.856 μs	85.856 μs	0 ns	ampere_sgemm_128x128_tn

Matmul = many FLOPs, fast
Softmax = few FLOPs, slow

softmax -> reductions, synchronization, global memory reads/writes

---

Problem (mixed_precision_accumulation): 1 point
Run the following code and commment on the (accuracy of the) results.

s = torch.tensor(0,dtype=torch.float32)for i in range(1000):
s += torch.tensor(0.01,dtype=torch.float32)
print(s)

s = torch.tensor(0,dtype=torch.float16)for i in range(1000):
s += torch.tensor(0.01,dtype=torch.float16)
print(s)

s = torch.tensor(0,dtype=torch.float32)for i in range(1000):
s += torch.tensor(0.01,dtype=torch.float16)
print(s)

s = torch.tensor(0,dtype=torch.float32)for i in range(1000):
x = torch.tensor(0.01,dtype=torch.float16)
s += x.type(torch.float32)
print(s)

Deliverable: A 2-3 sentence response.

1. FP32 Accumulation
Result: 10.00013351

2. FP16 Accumulation
Result: 9.95312500

3. Mixed Precision (FP32 Accumulator + FP16 Value)
Result: 10.00213623

4. Mixed Precision (Explicit Cast)
Result: 10.00213623

---

Problem (benchmarking_mixed_precision): 2 points
(a) Consider the ToyModel:

Suppose we are training the model on a GPU and that the model parameters are originally in FP32. We’d like to use autocasting mixed precision with FP16. What are the data types of:
• the model parameters within the autocast context,
• the output of the first feed-forward layer (ToyModel.fc1),
• the output of layer norm (ToyModel.ln),
• the model’s predicted logits,
• the loss,
• and the model’s gradients?Deliverable: The data types for each of the components listed above.

parameter dtype: torch.float32
input: torch.float32
after fc1: torch.float16
after relu: torch.float16
after layernorm: torch.float32
after fc2 (logits): torch.float16
logits outside model: torch.float16
loss dtype: torch.float32

GRADIENT DTYPES:
fc1.weight grad dtype: torch.float32
ln.weight grad dtype: torch.float32
ln.bias grad dtype: torch.float32
fc2.weight grad dtype: torch.float32

(b) You should have seen that FP16 mixed precision autocasting treats the layer normalization layer differently than the feed-forward layers. What parts of layer normalization are sensitive to mixed precision? If we use BF16 instead of FP16, do we still need to treat layer normalization differently? Why or why not?
Deliverable: A 2-3 sentence response.

mean and variance, which involve sum and division by small numbers

(c) Modify your benchmarking script to optionally run the model using mixed precision with BF16. Time the forward and backward passes with and without mixed-precision for each language model size described in §1.1.2. Compare the results of using full vs. mixed precision, and comment on any trends as model size changes. You may find the nullcontext no-op context manager to be useful.
Deliverable: A 2-3 sentence response with your timings and commentary

fwd only:

| Model  | Params | FP32 (ms) | BF16 (ms) | Speedup   |
| ------ | ------ | --------- | --------- | --------- |
| Small  | 128M   | 34.57     | 19.94     | 1.73×     |
| Medium | 423M   | 109.64    | 46.15     | 2.38×     |
| Large  | 969M   | 232.65    | 97.46     | 2.39×     |
| XL     | 1.99B  | 482.49    | 175.44    | 2.75×     |
| 2.35B  | 2.36B  | 458.62    | 112.58    | 4.07×     |


fwd + bwd :
| Model  | Params | FP32 (ms) | BF16 (ms) | Speedup   |
| ------ | ------ | --------- | --------- | --------- |
| Small  | 128M   | 108.51    | 56.47     | 1.92×     |
| Medium | 423M   | 320.68    | 149.26    | 2.15×     |
| Large  | 969M   | 706.12    | 311.31    | 2.27×     |
| XL     | 1.99B  | 1435.72   | 567.39    | 2.53×     |
| 2.35B  | 2.36B  | 1268.54   | 408.03    | 3.11×     |


---

Problem (memory_profiling): 4 points

Profile your forward pass, backward pass, and optimizer step of the 2.7B model from Table 1 with context lengths of 128, 256 and 512.

(running for 969M large model)

(a) Add an option to your profiling script to run your model through the memory profiler. It may be helpful to reuse some of your previous infrastructure (e.g., to activate mixed-precision, load specific model sizes, etc). Then, run your script to get a memory profile of the 2.7B model when either doing inference only (just forward pass) or a full training step. How do your memory timelines look like? Can you tell which stage is running based on the peaks you see?
Deliverable: Two images of the “Active memory timeline” of a 2.7B model, from the memory_viztool: one for the forward pass, and one for running a full training step (forward and backward passes, then optimizer step), and a 2-3 sentence response.

uv run python cs336_systems/memory_profiling.py --d_model 1280 --d_ff 5120 --n_layers 36 --n_heads 20 --batch_size 4 --vocab_size 10000 --warmup_steps 5 --n_steps 3 --max_seq_len 256 --precision bf16 --profile --output_file mem_fwd_256.pickle

uv run python cs336_systems/memory_profiling.py --d_model 1280 --d_ff 5120 --n_layers 36 --n_heads 20 --batch_size 4 --vocab_size 10000 --warmup_steps 5 --n_steps 3 --max_seq_len 256 --include_optimizer --precision bf16 --profile --output_file mem_fwd_bwd_opt_256.pickle


(b) What is the peak memory usage of each context length when doing a forward pass? What about when doing a full training step?
Deliverable: A table with two numbers per context length.

(c) Find the peak memory usage of the 2.7B model when using mixed-precision, for both a forward pass and a full optimizer step. Does mixed-precision significantly affect memory usage?
Deliverable: A 2-3 sentence response.

(d) Consider the 2.7B model. At our reference hyperparameters, what is the size of a tensor of activations in the Transformer residual stream, in single-precision? Give this size in MB (i.e., divide the number of bytes by 10242).
Deliverable: A 1-2 sentence response with your derivation.

(e) Now look closely at the “Active Memory Timeline” from pytorch.org/memory_viz of a memory snapshot of the 2.7B model doing a forward pass. When you reduce the “Detail” level, the tool hides the smallest allocations to the corresponding level (e.g., putting “Detail” at 10% only shows the 10% largest allocations). What is the size of the largest allocations shown? Looking through the stack trace, can you tell where those allocations come from?

---

Problem (pytorch_attention): 2 points

(a) Benchmark your attention implementation at different scales. Write a script that will:

(a) Fix the batch size to 8 and don’t use multihead attention (i.e. remove the head dimension).
(b) Iterate through the cartesian product of [16, 32, 64, 128] for the head embedding dimension dmodel, and [256, 1024, 4096, 8192, 16384] for the sequence length.
(c) Create random inputs Q, K, V for the appropriate size.
(d) Time 100 forward passes through attention using the inputs.
(e) Measure how much memory is in use before the backward pass starts, and time 100 backward passes.
(f) Make sure to warm up, and to call torch.cuda.synchronize() after each forward/backward pass.

Report the timings (or out-of-memory errors) you get for these configurations. At what size do you get out-of-memory errors? Do the accounting for the memory usage of attention in one of the smallest configurations you find that runs out of memory (you can use the equations for memory usage of Transformers from Assignment 1). How does the memory saved for backward change with the sequence length? What would you do to eliminate this memory cost?

Deliverable: A table with your timings, your working out for the memory usage, and a 1-2 paragraph response.

command: 

uv run python cs336_systems/benchmark_attention.py

output:

seq_len    | d_model    | fwd_time (ms)   | bwd_time (ms)   | mem (MB)        | status    
256        | 16         | 0.1188          | 2.5328          | 25.13           | OK        
256        | 32         | 0.1204          | 0.3838          | 26.00           | OK        
256        | 64         | 0.1215          | 0.3825          | 27.75           | OK        
256        | 128        | 0.1213          | 0.3794          | 31.25           | OK        
1024       | 16         | 0.4388          | 0.8691          | 150.75          | OK        
1024       | 32         | 0.4465          | 0.8893          | 151.25          | OK        
1024       | 64         | 0.4732          | 0.9210          | 158.25          | OK        
1024       | 128        | 0.5197          | 1.0231          | 172.25          | OK        
4096       | 16         | 6.6516          | 13.9358         | 2090.25         | OK        
4096       | 32         | 6.9035          | 14.1432         | 2092.25         | OK        
4096       | 64         | 7.1765          | 14.5369         | 2120.25         | OK        
4096       | 128        | 7.9176          | 15.7581         | 2176.25         | OK        
8192       | 16         | 26.9023         | 56.7155         | 8292.25         | OK        
8192       | 32         | 27.9918         | 57.6839         | 8264.25         | OK        
8192       | 64         | 29.0571         | 58.9540         | 8320.25         | OK        
8192       | 128        | 31.9183         | 64.1194         | 8432.25         | OK        
16384      | 16         | N/A             | N/A             | OOM             | FAIL      
16384      | 32         | N/A             | N/A             | OOM             | FAIL      
16384      | 64         | N/A             | N/A             | OOM             | FAIL      
16384      | 128        | N/A             | N/A             | OOM             | FAIL

mem ~ O(seq_len^2)
quadatric memory complexity
seq_len = 16384 might require ~ 32GB

attention matrix mem:
scores: (QK^T) = b * n_head * seq_len * seq_len * 4 bytes 
probs (softmax) = b * n_head * seq_len * seq_len * 4 bytes
attn (attention) = b * n_head * seq_len * d_model * 4 bytes

mem: O(seq_len^2)

---

Problem (torch_compile): 2 points
(a) Extend your attention benchmarking script to include a compiled version of your PyTorch implementation of attention, and compare its performance to the uncompiled version with the same configuration as the pytorch_attention problem above.
Deliverable: A table comparing your forward and backward pass timings for your compiled attention module with the uncompiled version from the pytorch_attention problem above.

(b) Now, compile your entire Transformer model in your end-to-end benchmarking script. How does
the performance of the forward pass change? What about the combined forward and backward
passes and optimizer steps?
Deliverable: A table comparing your vanilla and compiled Transformer model.


uv run python cs336_systems/benchmark_compiled_attention.py

seq_len    | d_model    | fwd_time (ms)   | cmp_fwd_time (ms)    | fwd_speedup  | bwd_time (ms)   | cmp_bwd_time (ms)    | bwd_speedup  | mem (MB)        | cmp_memory (MB) | status    
256        | 16         | 0.1201          | 0.1413               | 0.8502       | 0.3671          | 0.3157               | 1.1628       | 25.13           | 21.13           | OK        
256        | 32         | 0.1197          | 0.1815               | 0.6599       | 0.3868          | 0.3226               | 1.1991       | 26.00           | 22.00           | OK        
256        | 64         | 0.1204          | 0.1814               | 0.6636       | 0.3816          | 0.3262               | 1.1697       | 27.75           | 23.75           | OK        
256        | 128        | 0.1208          | 0.1818               | 0.6646       | 0.3838          | 0.3206               | 1.1972       | 31.25           | 27.25           | OK        
1024       | 16         | 0.4392          | 0.2978               | 1.4750       | 0.8708          | 0.5310               | 1.6397       | 150.75          | 83.75           | OK        
1024       | 32         | 0.4492          | 0.3063               | 1.4664       | 0.8896          | 0.5576               | 1.5953       | 151.25          | 87.25           | OK        
1024       | 64         | 0.4763          | 0.3346               | 1.4236       | 0.9195          | 0.6033               | 1.5242       | 158.25          | 94.25           | OK        
1024       | 128        | 0.5212          | 0.4039               | 1.2903       | 1.0674          | 0.7381               | 1.4461       | 172.25          | 108.25          | OK        
4096       | 16         | 6.6503          | 5.1474               | 1.2920       | 13.9352         | 9.4763               | 1.4705       | 2090.25         | 1054.25         | OK        
4096       | 32         | 6.9143          | 5.4317               | 1.2729       | 14.1503         | 9.6756               | 1.4625       | 2092.25         | 1068.25         | OK        
4096       | 64         | 7.1720          | 5.7118               | 1.2556       | 14.5493         | 10.0626              | 1.4459       | 2120.25         | 1096.25         | OK        
4096       | 128        | 7.9700          | 6.5800               | 1.2112       | 15.8304         | 11.6796              | 1.3554       | 2176.25         | 1152.25         | OK        
8192       | 16         | 26.9023         | 26.0188              | 1.0340       | 56.7300         | 39.4336              | 1.4386       | 8292.25         | 4140.25         | OK        
8192       | 32         | 28.0258         | 27.1994              | 1.0304       | 57.6825         | 40.6665              | 1.4184       | 8264.25         | 4168.25         | OK        
8192       | 64         | 29.1186         | 28.2625              | 1.0303       | 58.9873         | 41.8939              | 1.4080       | 8320.25         | 4224.25         | OK        
8192       | 128        | 32.0981         | 31.4579              | 1.0204       | 64.2457         | 48.4701              | 1.3255       | 8432.25         | 4336.25         | OK        
16384      | 16         | N/A             | N/A                  | N/A          | N/A             | N/A                  | N/A          | OOM             | OOM             | FAIL      
16384      | 32         | N/A             | N/A                  | N/A          | N/A             | N/A                  | N/A          | OOM             | OOM             | FAIL      
16384      | 64         | N/A             | N/A                  | N/A          | N/A             | N/A                  | N/A          | OOM             | OOM             | FAIL      
16384      | 128        | N/A             | N/A                  | N/A          | N/A             | N/A                  | N/A          | OOM             | OOM             | FAIL  

---

